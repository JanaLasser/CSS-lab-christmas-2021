{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99eba099-9a5e-4b31-a59d-40847a7ef66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from scipy.special import softmax\n",
    "from os.path import join\n",
    "from torch import cuda\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import sys\n",
    "import emoji_resources as er"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e0a58-e346-4121-9ce7-2dd7973cb956",
   "metadata": {},
   "source": [
    "According to the [model page](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment), the model can natively do the following languages, because it was trained on them:\n",
    "* ar (arabic)\n",
    "* en (english)\n",
    "* fr (french)\n",
    "* de (german)\n",
    "* hi (hindi)\n",
    "* it (italian)\n",
    "* es (spanish)\n",
    "* pt (portugues)\n",
    "\n",
    "The [paper](https://arxiv.org/pdf/2104.12250.pdf) says it can be used for more languages, but I couldn't figure out, which.\n",
    "\n",
    "The languages the underlying [xlm-roberta-model](https://arxiv.org/pdf/1911.02116.pdf) (see Appendix A) was trained on include:\n",
    "* ja (japanese)\n",
    "* ru (russian)\n",
    "* id (indonesian)\n",
    "* tr (turkish)\n",
    "* ko (korean)\n",
    "* th (thai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bacc94f2-9e17-4be3-81c1-a204a5e6b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in notebook, setting testing defaults\n"
     ]
    }
   ],
   "source": [
    "emojis = er.emojis\n",
    "try:\n",
    "    lang = sys.argv[1]\n",
    "    if lang == \"-f\":\n",
    "        print(\"running in notebook, setting testing defaults\")\n",
    "        lang = \"hi\"\n",
    "        emoji = \"ðŸ¤Œ\"\n",
    "        test = True\n",
    "    elif lang in er.languages:\n",
    "        try:\n",
    "            emoji = sys.argv[2]\n",
    "            emoji = emojis[emoji]\n",
    "        except IndexError:\n",
    "            print(\"no emoji supplied!\")\n",
    "        try:\n",
    "            test = sys.argv[3]\n",
    "            if test == \"test\":\n",
    "                test = True\n",
    "        except IndexError:\n",
    "            test = False\n",
    "    else:\n",
    "        print(f\"unknown language\")\n",
    "        \n",
    "except IndexError:\n",
    "    print(\"no language supplied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1406c9ac-d76c-4783-84a9-8a091ef353a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row.text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True)\n",
    "\n",
    "        return dict(input_ids=th.tensor(encoding[\"input_ids\"], dtype=th.long),\n",
    "                    attention_mask=th.tensor(encoding[\"attention_mask\"], dtype=th.long),\n",
    "                    token_type_ids=th.tensor(encoding[\"token_type_ids\"], dtype=th.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4ff41-c73e-4804-8823-07a212de68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"models/twitter-xlm-roberta-base-sentiment\" \n",
    "model = AutoModelForSequenceClassification\\\n",
    "        .from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5c3d4f7-1167-4f67-9d31-a202b388ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../data/tweets\"\n",
    "file = f\"tweets_language-{lang}_emoji-{emoji}_2019-01-01-to-2021-11-28.parquet.gzip\"\n",
    "\n",
    "if test:\n",
    "    df = pd.read_parquet(join(src, file))\n",
    "    df = df[0:10]\n",
    "else:\n",
    "    df = pd.read_parquet(join(src, file))\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "if test:\n",
    "    batch_size = 10\n",
    "else:\n",
    "    batch_size = 4096\n",
    "\n",
    "inference_set = InferenceDataset(df, tokenizer, max_token_len=128)\n",
    "inference_params = {'batch_size': batch_size, 'shuffle': False}\n",
    "inference_loader = DataLoader(inference_set, **inference_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c0cc047-b0f7-405e-94b1-abb9250b85e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 10\n",
      "/home/jana/anaconda3/envs/misinfo/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:380: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   negative   neutral  positive\n",
      "0  0.032637  0.028273  0.036788\n",
      "1  0.039159  0.029295  0.027809\n",
      "2  0.028249  0.039377  0.030254\n",
      "3  0.030703  0.031591  0.034855\n",
      "4  0.033584  0.030449  0.031835\n",
      "5  0.062768  0.034475  0.016023\n",
      "6  0.041121  0.031654  0.023742\n",
      "7  0.065914  0.026992  0.015719\n",
      "8  0.036633  0.029535  0.028600\n",
      "9  0.050708  0.029019  0.022240\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f\"running on device: {device}\")\n",
    "\n",
    "raw_pred, _, _ = trainer.prediction_loop(inference_loader, description=\"prediction\")\n",
    "scores = softmax(raw_pred)\n",
    "\n",
    "df['negative'] = scores[0:, 0]\n",
    "df['neutral'] = scores[0:, 1]\n",
    "df['positive'] = scores[0:, 2]\n",
    "\n",
    "\n",
    "if test:\n",
    "    print(df[['negative', 'neutral', 'positive',]])\n",
    "else:\n",
    "    dst = \"../data/sentiment\"\n",
    "    resname = f\"sentiment_language-{lang}_emoji-{emoji}_2019-01-01-to-2021-11-28.csv.gzip\"\n",
    "    df[[\"id\", \"negative\", \"neutral\", \"positive\"]]\\\n",
    "        .to_csv(join(dst, resname), index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
