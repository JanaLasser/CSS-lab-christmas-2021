{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99eba099-9a5e-4b31-a59d-40847a7ef66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from scipy.special import softmax\n",
    "from os.path import join\n",
    "from torch import cuda\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import sys\n",
    "import emoji_resources as er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bacc94f2-9e17-4be3-81c1-a204a5e6b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in notebook, setting testing defaults\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lang = sys.argv[1]\n",
    "    if lang == \"-f\":\n",
    "        print(\"running in notebook, setting testing defaults\")\n",
    "        lang = \"hi\"\n",
    "        emoji = \"ðŸ¤Œ\"\n",
    "        test = True\n",
    "    elif lang in er.languages:\n",
    "        try:\n",
    "            emoji = int(sys.argv[2])\n",
    "        except IndexError:\n",
    "            print(\"no emoji supplied!\")\n",
    "        try:\n",
    "            test = int(sys.argv[3])\n",
    "            if test == \"test\":\n",
    "                test = True\n",
    "        except IndexError:\n",
    "            test = False\n",
    "    else:\n",
    "        print(f\"unknown language\")\n",
    "        \n",
    "except IndexError:\n",
    "    print(\"no language supplied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1406c9ac-d76c-4783-84a9-8a091ef353a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row.text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True)\n",
    "\n",
    "        return dict(input_ids=th.tensor(encoding[\"input_ids\"], dtype=th.long),\n",
    "                    attention_mask=th.tensor(encoding[\"attention_mask\"], dtype=th.long),\n",
    "                    token_type_ids=th.tensor(encoding[\"token_type_ids\"], dtype=th.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2f4ff41-c73e-4804-8823-07a212de68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/twitter-xlm-roberta-base-sentiment/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file models/twitter-xlm-roberta-base-sentiment/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at models/twitter-xlm-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file models/twitter-xlm-roberta-base-sentiment/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "Didn't find file models/twitter-xlm-roberta-base-sentiment/tokenizer.json. We won't load it.\n",
      "Didn't find file models/twitter-xlm-roberta-base-sentiment/added_tokens.json. We won't load it.\n",
      "Didn't find file models/twitter-xlm-roberta-base-sentiment/tokenizer_config.json. We won't load it.\n",
      "loading file models/twitter-xlm-roberta-base-sentiment/sentencepiece.bpe.model\n",
      "loading file None\n",
      "loading file None\n",
      "loading file models/twitter-xlm-roberta-base-sentiment/special_tokens_map.json\n",
      "loading file None\n",
      "loading configuration file models/twitter-xlm-roberta-base-sentiment/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading configuration file models/twitter-xlm-roberta-base-sentiment/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"models/twitter-xlm-roberta-base-sentiment\" \n",
    "model = AutoModelForSequenceClassification\\\n",
    "        .from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5c3d4f7-1167-4f67-9d31-a202b388ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../data/tweets\"\n",
    "file = f\"tweets_language-{lang}_emoji-{emoji}_2019-01-01-to-2021-11-28.parquet.gzip\"\n",
    "\n",
    "if test:\n",
    "    df = pd.read_parquet(join(src, file))\n",
    "    df = df[0:10]\n",
    "else:\n",
    "    df = pd.read_parquet(join(src, file))\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "if test:\n",
    "    batch_size = 10\n",
    "else:\n",
    "    batch_size = 4096\n",
    "\n",
    "inference_set = InferenceDataset(df, tokenizer, max_token_len=128)\n",
    "inference_params = {'batch_size': batch_size, 'shuffle': False}\n",
    "inference_loader = DataLoader(inference_set, **inference_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c0cc047-b0f7-405e-94b1-abb9250b85e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 10\n",
      "/home/jana/anaconda3/envs/misinfo/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:380: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   negative   neutral  positive\n",
      "0  0.032637  0.028273  0.036788\n",
      "1  0.039159  0.029295  0.027809\n",
      "2  0.028249  0.039377  0.030254\n",
      "3  0.030703  0.031591  0.034855\n",
      "4  0.033584  0.030449  0.031835\n",
      "5  0.062768  0.034475  0.016023\n",
      "6  0.041121  0.031654  0.023742\n",
      "7  0.065914  0.026992  0.015719\n",
      "8  0.036633  0.029535  0.028600\n",
      "9  0.050708  0.029019  0.022240\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f\"running on device: {device}\")\n",
    "\n",
    "raw_pred, _, _ = trainer.prediction_loop(inference_loader, description=\"prediction\")\n",
    "scores = softmax(raw_pred)\n",
    "\n",
    "df['negative'] = scores[0:, 0]\n",
    "df['neutral'] = scores[0:, 1]\n",
    "df['positive'] = scores[0:, 2]\n",
    "\n",
    "\n",
    "if test:\n",
    "    print(df[['negative', 'neutral', 'positive',]])\n",
    "else:\n",
    "    dst = \"../data/sentiment\"\n",
    "    resname = f\"sentiment_language-{lang}_emoji-{emoji}_2019-01-01-to-2021-11-28.csv.gzip\"\n",
    "    df[[\"id\", \"negative\", \"neutral\", \"positive\"]]\\\n",
    "        .to_csv(join(dst, resname), index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
